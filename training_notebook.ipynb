{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Single Model Training Workflow ---\n",
      "Engineering features to match submission script...\n",
      "Training data shape: (8213, 41), Validation data shape: (756, 41)\n",
      "\n",
      "--- 5. Training Final Single Model ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9660\n",
      "[LightGBM] [Info] Number of data points in the train set: 8213, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.000022\n",
      "Training until validation scores don't improve for 300 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/st/8z9yjrgn4fq_j2pyt73wn7d00000gn/T/ipykernel_23069/3724230750.py:77: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_train = pd.read_csv(TRAIN_FILE_PATH, parse_dates=['date_id'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\tvalid_0's huber: 3.30481e-05\tvalid_0's custom_sharpe: 0.0764093\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's huber: 3.29196e-05\tvalid_0's custom_sharpe: 0.0765416\n",
      "\n",
      "--- 6. Training Final Model on All Data ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9671\n",
      "[LightGBM] [Info] Number of data points in the train set: 8990, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 0.000051\n",
      "Final model training complete.\n",
      "Saving final model to: /Users/rushilpatel/Downloads/hull-tactical-market-prediction/lgbm_model.pkl\n",
      "Model saved successfully.\n",
      "Saving feature list to: /Users/rushilpatel/Downloads/hull-tactical-market-prediction/selected_features.txt\n",
      "Feature list saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Make sure these paths are correct for your local setup.\n",
    "DATA_PATH = '/Users/rushilpatel/Downloads/hull-tactical-market-prediction/'\n",
    "OUTPUT_PATH = '/Users/rushilpatel/Downloads/hull-tactical-market-prediction/'\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_PATH, 'train.csv')\n",
    "MODEL_OUTPUT_PATH = os.path.join(OUTPUT_PATH, 'lgbm_model.pkl')\n",
    "FEATURES_OUTPUT_PATH = os.path.join(OUTPUT_PATH, 'selected_features.txt')\n",
    "DAYS_PER_YEAR = 252\n",
    "\n",
    "# --- SYNCHRONIZED WITH YOUR WORKING SUBMISSION SCRIPT ---\n",
    "ALLOCATION_SCALING_FACTOR = 7.0\n",
    "\n",
    "# --- Custom Metric Implementation ---\n",
    "def calculate_final_score(y_true, y_pred, df_for_metric):\n",
    "    \"\"\"Calculates the volatility- and return-penalized Sharpe ratio.\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    market_returns = df_for_metric['forward_returns'].values\n",
    "    risk_free_rate = df_for_metric['risk_free_rate'].values\n",
    "    \n",
    "    positions = np.clip(1.0 + y_pred * ALLOCATION_SCALING_FACTOR, 0, 2)\n",
    "    strategy_returns = risk_free_rate * (1 - positions) + positions * market_returns\n",
    "    strategy_excess_returns = strategy_returns - risk_free_rate\n",
    "    \n",
    "    clipped_ser = np.clip(strategy_excess_returns, -0.5, 0.5)\n",
    "    if len(clipped_ser) < 2: return 0.0\n",
    "    \n",
    "    geo_mean_strategy = np.expm1(np.mean(np.log1p(clipped_ser)))\n",
    "    vol_strategy = np.std(strategy_returns, ddof=1) * np.sqrt(DAYS_PER_YEAR)\n",
    "    if vol_strategy < 1e-6: return 0.0\n",
    "\n",
    "    market_excess_returns = market_returns - risk_free_rate\n",
    "    clipped_mer = np.clip(market_excess_returns, -0.5, 0.5)\n",
    "    geo_mean_market = np.expm1(np.mean(np.log1p(clipped_mer)))\n",
    "    vol_market = np.std(market_returns, ddof=1) * np.sqrt(DAYS_PER_YEAR)\n",
    "    \n",
    "    sharpe_ratio = (geo_mean_strategy / vol_strategy) * np.sqrt(DAYS_PER_YEAR)\n",
    "    vol_penalty = 1 + max(0, (vol_strategy / vol_market) - 1.2)\n",
    "    annualized_return_gap = max(0, (geo_mean_market - geo_mean_strategy) * DAYS_PER_YEAR)\n",
    "    return_penalty = 1 + (annualized_return_gap ** 2) / 100\n",
    "    \n",
    "    return min(sharpe_ratio / (vol_penalty * return_penalty), 1_000_000)\n",
    "\n",
    "def sharpe_eval_metric(y_true, y_pred):\n",
    "    score = calculate_final_score(y_true, y_pred, val_metric_df)\n",
    "    return 'custom_sharpe', score, True\n",
    "\n",
    "# --- SYNCHRONIZED WITH YOUR WORKING SUBMISSION SCRIPT ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    print(\"Engineering features to match submission script...\")\n",
    "    \n",
    "    df['P1_div_V1'] = df['P1'] / (df['V1'] + 1e-6)\n",
    "    df['I1_mul_E1'] = df['I1'] * df['E1']\n",
    "    lags = [1, 3, 5, 10]\n",
    "    features_to_lag = ['M1', 'V1', 'P1', 'S1']\n",
    "    for feature in features_to_lag:\n",
    "        for lag in lags:\n",
    "            df[f'{feature}_lag_{lag}'] = df[feature].shift(lag)\n",
    "            \n",
    "    return df\n",
    "\n",
    "val_metric_df = None\n",
    "\n",
    "def train_and_evaluate_model():\n",
    "    global val_metric_df\n",
    "    print(\"--- Starting Single Model Training Workflow ---\")\n",
    "\n",
    "    df_train = pd.read_csv(TRAIN_FILE_PATH, parse_dates=['date_id'])\n",
    "    df_train = df_train.sort_values('date_id')\n",
    "\n",
    "\n",
    "    base_features = [\n",
    "        'M1', 'M5', 'M6', 'E1', 'E5', 'E7', 'I1', 'I5', 'I7',\n",
    "        'P1', 'P5', 'P6', 'V1', 'V5', 'V8', 'V9',\n",
    "        'S1', 'S5', 'S8', 'S10', 'D1', 'D5', 'D8'\n",
    "    ]\n",
    "    target = 'market_forward_excess_returns'\n",
    "    \n",
    "    df_train_engineered = engineer_features(df_train)\n",
    "    \n",
    "    engineered_feature_names = ['P1_div_V1', 'I1_mul_E1'] + [f'{f}_lag_{l}' for f in ['M1', 'V1', 'P1', 'S1'] for l in [1, 3, 5, 10]]\n",
    "    final_features = base_features + engineered_feature_names\n",
    "\n",
    "    df_train_engineered.dropna(subset=[target], inplace=True)\n",
    "    \n",
    "    X = df_train_engineered[final_features].ffill().bfill()\n",
    "    y = df_train_engineered[target]\n",
    "\n",
    "    # --- NEW: More Robust Train/Validation Split ---\n",
    "    validation_size = 252 * 3  # Use last 3 years for validation\n",
    "    gap_size = 21 # 1-month gap to prevent data leakage\n",
    "    \n",
    "    train_end_index = len(X) - validation_size - gap_size\n",
    "    val_start_index = len(X) - validation_size\n",
    "\n",
    "    X_train, y_train = X.iloc[:train_end_index], y.iloc[:train_end_index]\n",
    "    X_val, y_val = X.iloc[val_start_index:], y.iloc[val_start_index:]\n",
    "    \n",
    "    val_metric_df = df_train_engineered.iloc[val_start_index:]\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}, Validation data shape: {X_val.shape}\")\n",
    "    \n",
    "    # --- Final Model Configuration ---\n",
    "    print(\"\\n--- 5. Training Final Single Model ---\")\n",
    "    lgbm = lgb.LGBMRegressor(\n",
    "        objective='huber',\n",
    "        random_state=42,\n",
    "        n_estimators=4000,\n",
    "        learning_rate=0.005, # Slow learning\n",
    "        num_leaves=16,\n",
    "        min_child_samples=200, # Strong constraint\n",
    "        subsample=0.6,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1\n",
    "    )\n",
    "\n",
    "    lgbm.fit(X_train, y_train,\n",
    "             eval_set=[(X_val, y_val)],\n",
    "             eval_metric=sharpe_eval_metric,\n",
    "             callbacks=[lgb.early_stopping(300, verbose=True), lgb.log_evaluation(300)])\n",
    "\n",
    "    # --- Retrain and Save Final Model ---\n",
    "    print(\"\\n--- 6. Training Final Model on All Data ---\")\n",
    "    final_model = lgb.LGBMRegressor(**lgbm.get_params())\n",
    "    best_iter = lgbm.best_iteration_ if lgbm.best_iteration_ is not None and lgbm.best_iteration_ > 0 else 1\n",
    "    final_model.set_params(n_estimators=best_iter)\n",
    "    final_model.fit(X, y)\n",
    "    print(\"Final model training complete.\")\n",
    "\n",
    "    print(f\"Saving final model to: {MODEL_OUTPUT_PATH}\")\n",
    "    with open(MODEL_OUTPUT_PATH, 'wb') as f:\n",
    "        pickle.dump(final_model, f)\n",
    "    print(\"Model saved successfully.\")\n",
    "    \n",
    "    print(f\"Saving feature list to: {FEATURES_OUTPUT_PATH}\")\n",
    "    with open(FEATURES_OUTPUT_PATH, 'w') as f:\n",
    "        for feature in X.columns:\n",
    "            f.write(f\"{feature}\\n\")\n",
    "    print(\"Feature list saved successfully.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "    train_and_evaluate_model()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house_prices_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
